# ğŸ“ˆ Linear Regression - YZM212 4. Laboratuvar Ã‡alÄ±ÅŸmasÄ±

Bu Ã§alÄ±ÅŸmada, iÅŸ deneyimi (yÄ±l) ile maaÅŸ arasÄ±ndaki doÄŸrusal iliÅŸkiyi modellemek amacÄ±yla **Linear Regression** algoritmasÄ± Ã¼Ã§ farklÄ± ÅŸekilde uygulanmÄ±ÅŸtÄ±r:

1. **Least Squares Estimation (LSE)** yÃ¶ntemiyle manuel Ã§Ã¶zÃ¼m
2. **Gradient Descent** algoritmasÄ± ile parametre gÃ¼ncelleme
3. **Scikit-learn kÃ¼tÃ¼phanesi** kullanÄ±larak hazÄ±r modelle Ã§Ã¶zÃ¼m

---

## ğŸ“Š KullanÄ±lan Veri Seti

- **Kaynak:** [Kaggle - Simple Linear Regression Salary Data](https://www.kaggle.com/datasets?tags=13405-Linear+Regression&utm_source=chatgpt.com)
- **Ã–zellikler:**
  - `YearsExperience`: BaÄŸÄ±msÄ±z deÄŸiÅŸken (iÅŸ deneyimi, yÄ±l)
  - `Salary`: BaÄŸÄ±mlÄ± deÄŸiÅŸken (maaÅŸ)
- **Format:** CSV

---

## ğŸ§® Teorik Bilgi

### ğŸ”¹ Linear Regression
AmaÃ§, veri noktalarÄ± arasÄ±nda en uygun doÄŸrusal iliÅŸkiyi (doÄŸruyu) bulmaktÄ±r. Genel formÃ¼lÃ¼:

\[
\hat{y} = \theta_0 + \theta_1 x
\]

### ğŸ”¹ Least Squares Estimation (LSE)
KapalÄ± formÃ¼l ile en uygun aÄŸÄ±rlÄ±klar ÅŸu ÅŸekilde hesaplanÄ±r:

\[
\theta = (X^T X)^{-1} X^T y
\]

### ğŸ”¹ Gradient Descent
AmaÃ§, hata fonksiyonunu minimize edecek aÄŸÄ±rlÄ±klarÄ± iteratif olarak bulmaktÄ±r.

\[
\theta_j := \theta_j - \alpha \cdot \frac{1}{m} \sum (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}
\]

---

## âš™ï¸ KullanÄ±lan KÃ¼tÃ¼phaneler

```bash
numpy
pandas
matplotlib
scikit-learn
